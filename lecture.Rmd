---
title: "Randomly Forested"
output: html_notebook
---


# Machine Learning v Inferential Statistics

What does the term machine learning mean to you?

<iframe width="560" height="315" src="https://www.youtube.com/embed/R9OHn5ZF4Uo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

- Linear Regression has it's origins in the first decade of the 19th century.
    - Method of Least Squares by Legendre (1805), Gauss (1809)
    - In the early 20th century, this idea was debated and extended by mathematicians such as Yule, Pearson, and Fisher.
    - In the 50's and 60's completing a single regression could take hours.
- Logistic Regression has it's origins in the 1830s and 1840s. 
    - The underlying idea was developed independently several times in different fields of study.
    - Early applications included modeling autocatalytic reactions in chemistry and population growth.
    - Bliss, Gaddum, and Fisher (he is everywhere!) extended these ideas in the 1930s. This work led to the probit model.
        - Probit is a combination of the words probability and unit, prob + it
    - 1940s when Berkson developed the logit function in 1944 which would eventually replace the probit function.
        - Logit or log-odds is the logarithm of the odds p/(1-p) where p is the probability.
        - Remember how I told you there was a relationship between the odds ratio and logistic regression?
        - Well, that's it!
- Although linear and logistic regression are POWERFUL techniques, they hard to apply to increasingly complex data.
    - They are entirely appropriate in cases where the data is constrained in terms of complexity AND suitable domain experts are available.
    - They can mislead when working with large data sets because it so easy to achieve statistical significance, which is often seen as a hallmark of a valid model.

# Last Week's Lab

- Question: How hard did this feel?

## Setup

```{r message=FALSE, warning=FALSE}
library(knitr)
library(tidyverse)
strokes <- read_csv("data/strokes.csv") %>% filter(gender != "Other")
```

## Data - Strokes

Source: [Kaggle Stroke Prediction Dataset](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset)

```{r paged.print=TRUE}
strokes
```

1. id: unique patient identifier
2. gender: "Male", "Female" or "Other"
3. age: age of the patient
4. hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension
5. heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease
6. ever_married: "No" or "Yes"
7. work_type: "children", "Govt_jov", "Never_worked", "Private" or "Self-employed"
8. residence_type: "Rural" or "Urban"
9. avg_glucose_level: average glucose level in blood
10. bmi: body mass index
11. smoking_status: "formerly smoked", "never smoked", "smokes" or "Unknown"*
12. stroke: 1 if the patient had a stroke or 0 if not

*Note: "Unknown" in smoking_status means that the information is unavailable for this patient

## Lab

### Task 01

- Question: The data includes some information about the employment of the patients. Which work type is the most prevalent in this data set?
- Answer: Private

To prevent confusion, your options below are identical to what you should have found in your data. In Canvas complete this task with one of the following:

    1. children
    2. Govt_job
    3. Never_worked
    4. Private
    5. Self-employed

Helpful Hints: The entries in the `work_type` column are formatted oddly, but that's OK. There's always _something_ funny about a data set.

```{r}
strokes %>%
  count(work_type) %>%
  arrange(desc(n))
```

### Task 02

- Question: Use R to calculate the percent of patients who are children (based on work type/Task 01).
- Answer: 13.4%

Helpful Hints:

- In other words, divide the work_type count by the total.
- And yes, you CAN do this without R, but the point is to do this using R.
- And the easiest way is to calculate the percent for all of the work types.

```{r}
strokes %>%
  count(work_type) %>%
  mutate(100*n/nrow(strokes))

## Or, less elegantly
strokes %>%
  count(work_type) %>%
  mutate(100*n/5109)
```


### Task 03

- Question: Which patients are more likely to have had a stroke, men or women? 
- Answer: Of course, the gentlemen win this one.

Helpful Hints:

- We want to calculate the percentage of patients with a stroke.
    - This is just like what we did with the titanic data, but we calculated the percent of passengers who died.
    - But it is the EXACT same idea.

```{r}
strokes %>%
  group_by(gender) %>%
  summarize(
    n = n(),
    strokes = sum(stroke),
    risk = strokes/n*100
  )
```

### Task 04

- Question: Do men with hypertension have a higher risk of stroke than women with hypertension?
- Answer: And this time, the ladies take the "win".

- This is ALMOST the same question.
- All we need to do is to filter down to those with hypertension.
- Filter should come before group by.

```{r}
strokes %>%
  filter(hypertension == 1) %>%
  group_by(gender) %>%
  summarize(
    n = n(),
    strokes = sum(stroke),
    risk = strokes/n*100
  )
```

### Task 05

- Question: How many patients are over the age of 65?
- Answer: 965

```{r}
strokes %>% filter(age > 65)
```

If this seems weird, it was. It was originally the first step in something more complex that I felt guilty about after the midterm and the fact that I had just canceled class. So, yeah. Weird.

### Task 06

- Question: What is the risk ratio for stroke for men compared to women?
- Answer: 1.084926

Helpful Hint:

- It may help to calculate this all as two steps.
- First calculate the risk of stroke for men and women.
    - Save this in a table called risk.
- Then calculate the risk ratio.
- And remember, if you can't figure out the fancy code for the risk ratio but you can figure out the gender-specific risk, you can just use R like a calculator and calculate the risk ratio manually.

```{r}
## Go back to a previous question:
risk_gender <-
  strokes %>%
  group_by(gender) %>%
  summarize(
    n = n(),
    strokes = sum(stroke),
    risk = strokes/n*100
  )
risk_gender

## Inelegant, but correct:
## I know, I didn't show you this.
## But did anyone think of it?
5.11/4.71

## OK, we can code this. Seriously, we can.
## And, yes, dplyr tends to suppress long annoying fractions.
## This uses a touch of base r style coding.
risk_gender %>%
  summarize(
    risk_ratio = risk[gender == "Male"]/risk[gender == "Female"]
  )

## Or the totally 100% dplyr style.
## Which is verbose. Sorry.
risk_gender %>%
  summarize(
    risk_ratio = max(if_else(gender == "Male", risk, 0))/max(if_else(gender == "Female", risk, 0))
  )

## Which, if we break it down a little:
risk_gender %>%
  summarize(
    male_risk = max(if_else(gender == "Male", risk, 0)),
    female_risk = max(if_else(gender == "Female", risk, 0)),
    risk_ratio = male_risk/female_risk
  )
```

- The awesome thing about a programming language is that there are **twelve** ways to solve nearly every problem.
- The sucky thing about programming is that there are **twelve** ways to solve nearly every problem.
- The number of possible solutions increases as the problems become more complex.
- That makes it hard to learn. I'm sorry.

### Task 07

- Question: What is the odds ratio for for stroke for men compared to women?
- Answer: 1.088


```{r}
## I'm only going to show you one answer to this.
## But you can, just like with risk ratio, solve this umpteen different ways.

odds_gender <-
  strokes %>%
  group_by(gender) %>%
  summarize(
    n = n(),
    strokes = sum(stroke),
    not_strokes = sum(!stroke),
    odds = strokes/not_strokes
  )
odds_gender

## Inelegant, but correct:
## I know, I didn't show you this.
## But did anyone think of it?
0.0538/0.0494

## PLEASE
## PLEASE SEE HOW THE RATIO HERE IS NEARLY IDENTICAL.
## PLEASE

odds_gender %>%
  summarize(
    odds_ratio = odds[gender == "Male"]/odds[gender == "Female"]
  )
```

- This literally means that for every female who has a stroke, 1.089 men have a stroke. Which, ummm, not OK to cut people up like that.
- So we often express it in other terms.
- Per 100: For every 100 women who have a stroke, nearly 109 men have a stroke.
    - Yes, I rounded.
    - But this is easier to understand.

### Task 08

- Question: Demonstrate that the same odds ratio can be obtained via logistic regression. Build a model for risk stroke as a function of gender. What is the coefficient for genderMale?
- Answer: OMG OMG OMG 1.09 which is basically the same as 1.088

```{r}
model_gender <- glm(stroke~gender, family = binomial, data = strokes)
summary(model_gender)


exp(model_gender$coefficients)
```

# Return To Titanic

```{r}
test <- read_csv("data/titanic-test-clean.csv")

titanic <- read_csv("data/titanic-train-clean.csv") %>%
  mutate(died = !survived) %>%
  select(-survived)
titanic
```

- Last week we built TWO logistic regression models.
- This week, I want to go a bit further.
- Above, I told you that logistic regression was just the log odds.

So, why not use linear regression to model this?

```{r}
titanic %>%
  mutate(prob = as.numeric(died)) %>%
  ggplot(aes(x = sex, y = prob)) +
  geom_jitter()
```

- Where would my line go, exactly? It doesn't make any sense.
- In linear regression, residuals must be normally distributed.
- There's nowhere on this graph where that can be true.
- And there's no great way to plot this in general.
- We can, in fairness, plot numerics in a manner that is meanginful.

```{r}
titanic %>%
  mutate(prob = as.numeric(died)) %>%
  ggplot(aes(x = age, y = prob)) +
  geom_point(alpha = .25) +
  geom_smooth(
    method = "glm",
    se = FALSE,
    fullrange = TRUE,
    method.args = list(family = "binomial")
  )

##And we can stratify this by facet_wrap.
```

- Clearly, the odds of died as a male are "worse" than for women.
- And this is hard to understand too. For every 1/3 of a female who dies, over four men die? I mean, OK, but, it still isn't OK to cut people up.
- Per 100: For every 35 female who died, 429 males died. Yikes.
- So the logs odds ratio is just:

```{r}
odds <-
  titanic %>%
  group_by(sex) %>%
  summarize(
    n = n(),
    died = sum(died),
    survived = n-died, ## or sum(!died)
    odds = died/survived
  )
odds
```

And so the odds ratio is:

```{r}
4.29/.348
```

Or we could be a bit more formal:

```{r}
odds %>%
  summarize(
    odds[sex == "male"]/odds[sex == "female"]
  )
```

Which is basically the same thing. Finally the log of our odds ratio:


```{r}
log(12.32759)
```

Now, let's look at a model:

```{r}
model_sex <- glm(died~sex, family = binomial, data = titanic)
summary(model_sex)
```

- The estimate for sexmale matches our log odds ratio calculation.
- But there's a lot more here.
- Null deviance (1186.7 on 890 degrees of freedom): How well does the intercept describe the model?
- Residual deviance (917.8 on 889 degrees of freedom): How well does the proposed model describe the model?
- Lower is better.
- The bigger the difference between the Null deviance and the Residual deviance, the better.
- And, last week, we used this model to classify.

```{r}
titanic$fitted <-
  predict(newdata = titanic %>% mutate(died = NA), model_sex)

ggplot(titanic, aes(x = exp(fitted))) + geom_histogram()

titanic <- 
  titanic %>%
  mutate(
    predicted = case_when(exp(fitted) > 1~1, TRUE~0)
  )

titanic %>%
  group_by(died) %>%
  summarize(
    predicted_died = sum(if_else(predicted == 1, 1, 0)),
    predicted_survived = sum(if_else(predicted == 0, 1, 0))
  )
```

- Now, let's build a more complex model.
- Let's see this in action.

```{r}
model_sex_age <- glm(died~sex+age, family = binomial, data = titanic)
summary(model_sex_age)
```

By adding age to the model, Null and Residual deviance both decreased (good).

```{r}
titanic$fitted <-
  predict(newdata = titanic %>% mutate(died = NA), model_sex_age)

titanic <- 
  titanic %>%
  mutate(
    predicted = case_when(exp(fitted) > 1~1, TRUE~0)
  )

titanic %>%
  group_by(died) %>%
  summarize(
    predicted_died = sum(if_else(predicted == 1, 1, 0)),
    predicted_survived = sum(if_else(predicted == 0, 1, 0))
  )
```

- Our model is more precise (see Null/Residual deviance).
- But our confusion matrix hasn't changed at all.
- Which, actually, makes sense.

```{r}
ggplot(titanic, aes(x = exp(fitted))) + geom_histogram()
```

- Although we _did_ create a better model, our classification isn't any better.
- But maybe we can help.

```{r}
model_sex_age_class <-
  glm(died~sex + age + pclass, family = binomial, data = titanic)
summary(model_sex_age_class)

titanic$fitted <-
  predict(newdata = titanic %>% mutate(died = NA), model_sex_age_class)

titanic <- 
  titanic %>%
  mutate(
    predicted = case_when(exp(fitted) > 1~1, TRUE~0)
  )

titanic %>%
  group_by(died) %>%
  summarize(
    predicted_died = sum(if_else(predicted == 1, 1, 0)),
    predicted_survived = sum(if_else(predicted == 0, 1, 0))
  )
```

So, huh?

```{r}
ggplot(titanic, aes(x = exp(fitted))) +
  geom_density() +
  geom_vline(aes(xintercept = 1), color = "red", width = 2) +
  facet_wrap(~pclass)
```

- Did I mention that there's a part of this that is ENTIRELY arbitrary?
- Why did I do this?

```{r eval=FALSE}
titanic <- 
  titanic %>%
  mutate(
    predicted = case_when(exp(fitted) > 1~1, TRUE~0)
  )
```

- This cut-point could be anywhere.
